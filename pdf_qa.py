# -*- coding: utf-8 -*-
"""Pdf_QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1taV2fw88tiWBE2VK5RW1b0wR6XsV1dwL
"""

# Install necessary libraries
!pip install transformers torch PyPDF2 langchain langchain-community sentence-transformers chromadb streamlit pyngrok huggingface_hub

# Set Hugging Face token
import os
os.environ['HUGGINGFACE_TOKEN'] = "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"

# Login to Hugging Face
from huggingface_hub import login
login(token=os.getenv('HUGGINGFACE_TOKEN'))

# Commented out IPython magic to ensure Python compatibility.
# %%writefile pdf_qa_app.py
# # Import libraries
# import streamlit as st
# from PyPDF2 import PdfReader
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain_community.embeddings import SentenceTransformerEmbeddings
# from langchain_community.vectorstores import Chroma
# from langchain_community.llms import HuggingFacePipeline
# from langchain.chains import RetrievalQA
# from pyngrok import ngrok
# import warnings
# import logging
# 
# # Suppress Streamlit warnings
# logging.getLogger('streamlit').setLevel(logging.ERROR)
# 
# # Ignore warnings
# warnings.filterwarnings("ignore")
# 
# 
# # Load the model and tokenizer
# checkpoint = "meta-llama/Llama-3.2-1B"
# tokenizer = AutoTokenizer.from_pretrained(checkpoint, token='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')  # Replace with your token
# base_model = AutoModelForCausalLM.from_pretrained(checkpoint, token='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')  # Replace with your token
# 
# @st.cache_resource
# def llm_pipeline():
#     pipe = pipeline(
#         'text-generation',
#         model=base_model,
#         tokenizer=tokenizer,
#         max_length=256,
#         do_sample=True,
#         temperature=0.3,
#         top_p=0.95
#     )
#     local_llm = HuggingFacePipeline(pipeline=pipe)
#     return local_llm
# 
# @st.cache_resource
# def qa_llm():
#     llm = llm_pipeline()
#     embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
#     db = Chroma(persist_directory="db", embedding_function=embeddings)
#     retriever = db.as_retriever()
#     qa = RetrievalQA.from_chain_type(
#         llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
#     return qa
# 
# def extract_text_from_pdf(pdf_file):
#     pdf_reader = PdfReader(pdf_file)
#     text = ""
#     for page in range(len(pdf_reader.pages)):
#         text += pdf_reader.pages[page].extract_text()
#     return text
# 
# def process_answer(instruction):
#     qa = qa_llm()
#     generated_text = qa(instruction)
#     answer = generated_text['result']
#     return answer, generated_text
# 
# def main():
#     st.title("Search Your PDF üê¶üìÑ")
# 
#     with st.expander("About the App"):
#         st.markdown("""This is a Generative AI powered Question and Answering app that responds to questions about your PDF File.""")
# 
#     uploaded_pdf = st.file_uploader("Upload your PDF", type="pdf")
# 
#     if uploaded_pdf is not None:
#         with st.spinner("Extracting text from PDF..."):
#             pdf_text = extract_text_from_pdf(uploaded_pdf)
#             st.success("Text extracted from PDF!")
# 
#         question = st.text_area("Enter your Question")
# 
#         if st.button("Ask"):
#             st.info("Your Question: " + question)
#             st.info("Your Answer")
#             answer, metadata = process_answer(question)
#             st.write(answer)
#             st.write(metadata)
# 
# if __name__ == '__main__':
#     main()
# 
#

!pip install --upgrade pyngrok
from pyngrok import ngrok
ngrok.set_auth_token("2n6Wbn6xiLAsyySCmFSos1gG0Ja_68ctrYXtCfCGaJkPZ896u")

# # Terminate all existing ngrok tunnels
ngrok.kill()

# # Create a new ngrok tunnel, explicitly specifying HTTP protocol
public_url = ngrok.connect(8501, proto="http") # Specify protocol as "http"
print(f"Streamlit App URL: {public_url}")

!pip install streamlit
import subprocess

# Run Streamlit app
subprocess.Popen(['streamlit', 'run', 'pdf_qa_app.py'])